{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "973ea42d-daf8-4dde-b139-9b8a91923835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, average_precision_score\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b322c996-678b-475b-a287-6e6b7bf3cec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"combined_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "973e9b94-7a18-41ab-872f-f6b28cc77b77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>Corr2</th>\n",
       "      <th>Diss_sim2</th>\n",
       "      <th>Homogen2</th>\n",
       "      <th>Contrast2</th>\n",
       "      <th>Energy3</th>\n",
       "      <th>Corr3</th>\n",
       "      <th>Diss_sim3</th>\n",
       "      <th>Contrast3</th>\n",
       "      <th>Energy4</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.192964</td>\n",
       "      <td>-0.019705</td>\n",
       "      <td>0.026618</td>\n",
       "      <td>0.351818</td>\n",
       "      <td>0.298722</td>\n",
       "      <td>-0.107279</td>\n",
       "      <td>-0.015158</td>\n",
       "      <td>0.003892</td>\n",
       "      <td>0.524406</td>\n",
       "      <td>0.039238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.919516</td>\n",
       "      <td>7.661608</td>\n",
       "      <td>0.127554</td>\n",
       "      <td>98.646275</td>\n",
       "      <td>0.015894</td>\n",
       "      <td>0.829719</td>\n",
       "      <td>11.224156</td>\n",
       "      <td>208.034439</td>\n",
       "      <td>0.107351</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.157366</td>\n",
       "      <td>-0.003816</td>\n",
       "      <td>-0.032858</td>\n",
       "      <td>0.196809</td>\n",
       "      <td>0.330078</td>\n",
       "      <td>0.383103</td>\n",
       "      <td>0.176619</td>\n",
       "      <td>0.198241</td>\n",
       "      <td>0.298677</td>\n",
       "      <td>0.024531</td>\n",
       "      <td>...</td>\n",
       "      <td>0.977489</td>\n",
       "      <td>4.184100</td>\n",
       "      <td>0.220152</td>\n",
       "      <td>29.967895</td>\n",
       "      <td>0.021490</td>\n",
       "      <td>0.947064</td>\n",
       "      <td>6.401126</td>\n",
       "      <td>70.011755</td>\n",
       "      <td>0.107569</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.161836</td>\n",
       "      <td>-0.049620</td>\n",
       "      <td>-0.011000</td>\n",
       "      <td>0.348439</td>\n",
       "      <td>0.330918</td>\n",
       "      <td>0.024357</td>\n",
       "      <td>0.014045</td>\n",
       "      <td>-0.043150</td>\n",
       "      <td>0.681322</td>\n",
       "      <td>0.188252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.958037</td>\n",
       "      <td>4.263552</td>\n",
       "      <td>0.213115</td>\n",
       "      <td>30.323549</td>\n",
       "      <td>0.023645</td>\n",
       "      <td>0.897296</td>\n",
       "      <td>6.670947</td>\n",
       "      <td>74.221170</td>\n",
       "      <td>0.124114</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.111617</td>\n",
       "      <td>-0.046071</td>\n",
       "      <td>-0.010727</td>\n",
       "      <td>0.289515</td>\n",
       "      <td>0.345447</td>\n",
       "      <td>-0.014513</td>\n",
       "      <td>-0.073970</td>\n",
       "      <td>-0.052690</td>\n",
       "      <td>0.628684</td>\n",
       "      <td>0.210376</td>\n",
       "      <td>...</td>\n",
       "      <td>0.925150</td>\n",
       "      <td>5.723648</td>\n",
       "      <td>0.166492</td>\n",
       "      <td>54.709969</td>\n",
       "      <td>0.020782</td>\n",
       "      <td>0.828209</td>\n",
       "      <td>8.687137</td>\n",
       "      <td>125.530404</td>\n",
       "      <td>0.123403</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.064389</td>\n",
       "      <td>-0.069336</td>\n",
       "      <td>-0.089412</td>\n",
       "      <td>0.319757</td>\n",
       "      <td>0.169375</td>\n",
       "      <td>-0.006689</td>\n",
       "      <td>-0.036932</td>\n",
       "      <td>-0.051993</td>\n",
       "      <td>0.562059</td>\n",
       "      <td>0.139916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.978757</td>\n",
       "      <td>3.995765</td>\n",
       "      <td>0.230973</td>\n",
       "      <td>28.129354</td>\n",
       "      <td>0.023903</td>\n",
       "      <td>0.951120</td>\n",
       "      <td>6.041322</td>\n",
       "      <td>64.345599</td>\n",
       "      <td>0.113598</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4184</th>\n",
       "      <td>0.296233</td>\n",
       "      <td>-0.173819</td>\n",
       "      <td>-0.132807</td>\n",
       "      <td>0.344676</td>\n",
       "      <td>0.304321</td>\n",
       "      <td>0.397681</td>\n",
       "      <td>0.205759</td>\n",
       "      <td>0.005951</td>\n",
       "      <td>0.303051</td>\n",
       "      <td>-0.060433</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999487</td>\n",
       "      <td>0.607368</td>\n",
       "      <td>0.749175</td>\n",
       "      <td>1.201333</td>\n",
       "      <td>0.137047</td>\n",
       "      <td>0.998722</td>\n",
       "      <td>0.995182</td>\n",
       "      <td>2.996365</td>\n",
       "      <td>0.203269</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4185</th>\n",
       "      <td>0.396976</td>\n",
       "      <td>-0.081235</td>\n",
       "      <td>-0.149095</td>\n",
       "      <td>0.266616</td>\n",
       "      <td>0.361952</td>\n",
       "      <td>0.590082</td>\n",
       "      <td>0.108408</td>\n",
       "      <td>0.333073</td>\n",
       "      <td>0.347843</td>\n",
       "      <td>-0.034620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.991617</td>\n",
       "      <td>2.392191</td>\n",
       "      <td>0.403972</td>\n",
       "      <td>14.914763</td>\n",
       "      <td>0.045681</td>\n",
       "      <td>0.980559</td>\n",
       "      <td>3.579858</td>\n",
       "      <td>34.599094</td>\n",
       "      <td>0.129531</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4186</th>\n",
       "      <td>0.304681</td>\n",
       "      <td>-0.134328</td>\n",
       "      <td>-0.017254</td>\n",
       "      <td>0.295696</td>\n",
       "      <td>0.609226</td>\n",
       "      <td>0.225363</td>\n",
       "      <td>0.309966</td>\n",
       "      <td>0.270593</td>\n",
       "      <td>0.506838</td>\n",
       "      <td>-0.128632</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998685</td>\n",
       "      <td>1.155074</td>\n",
       "      <td>0.648774</td>\n",
       "      <td>4.162545</td>\n",
       "      <td>0.132761</td>\n",
       "      <td>0.996455</td>\n",
       "      <td>1.920197</td>\n",
       "      <td>11.243822</td>\n",
       "      <td>0.181156</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4187</th>\n",
       "      <td>0.297888</td>\n",
       "      <td>-0.202946</td>\n",
       "      <td>-0.122310</td>\n",
       "      <td>0.371556</td>\n",
       "      <td>0.635890</td>\n",
       "      <td>0.387773</td>\n",
       "      <td>0.210122</td>\n",
       "      <td>0.073176</td>\n",
       "      <td>0.410118</td>\n",
       "      <td>-0.056148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999119</td>\n",
       "      <td>0.628868</td>\n",
       "      <td>0.756240</td>\n",
       "      <td>1.758523</td>\n",
       "      <td>0.132519</td>\n",
       "      <td>0.997725</td>\n",
       "      <td>1.036403</td>\n",
       "      <td>4.543673</td>\n",
       "      <td>0.201567</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4188</th>\n",
       "      <td>0.430524</td>\n",
       "      <td>-0.150791</td>\n",
       "      <td>-0.053603</td>\n",
       "      <td>0.215542</td>\n",
       "      <td>0.409507</td>\n",
       "      <td>0.229271</td>\n",
       "      <td>0.373093</td>\n",
       "      <td>0.009041</td>\n",
       "      <td>0.233875</td>\n",
       "      <td>-0.025062</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999364</td>\n",
       "      <td>0.327712</td>\n",
       "      <td>0.840301</td>\n",
       "      <td>0.369280</td>\n",
       "      <td>0.194667</td>\n",
       "      <td>0.998609</td>\n",
       "      <td>0.546002</td>\n",
       "      <td>0.808338</td>\n",
       "      <td>0.235993</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4189 rows × 272 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0     0.192964 -0.019705  0.026618  0.351818  0.298722 -0.107279 -0.015158   \n",
       "1     0.157366 -0.003816 -0.032858  0.196809  0.330078  0.383103  0.176619   \n",
       "2     0.161836 -0.049620 -0.011000  0.348439  0.330918  0.024357  0.014045   \n",
       "3     0.111617 -0.046071 -0.010727  0.289515  0.345447 -0.014513 -0.073970   \n",
       "4     0.064389 -0.069336 -0.089412  0.319757  0.169375 -0.006689 -0.036932   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4184  0.296233 -0.173819 -0.132807  0.344676  0.304321  0.397681  0.205759   \n",
       "4185  0.396976 -0.081235 -0.149095  0.266616  0.361952  0.590082  0.108408   \n",
       "4186  0.304681 -0.134328 -0.017254  0.295696  0.609226  0.225363  0.309966   \n",
       "4187  0.297888 -0.202946 -0.122310  0.371556  0.635890  0.387773  0.210122   \n",
       "4188  0.430524 -0.150791 -0.053603  0.215542  0.409507  0.229271  0.373093   \n",
       "\n",
       "             7         8         9  ...     Corr2  Diss_sim2  Homogen2  \\\n",
       "0     0.003892  0.524406  0.039238  ...  0.919516   7.661608  0.127554   \n",
       "1     0.198241  0.298677  0.024531  ...  0.977489   4.184100  0.220152   \n",
       "2    -0.043150  0.681322  0.188252  ...  0.958037   4.263552  0.213115   \n",
       "3    -0.052690  0.628684  0.210376  ...  0.925150   5.723648  0.166492   \n",
       "4    -0.051993  0.562059  0.139916  ...  0.978757   3.995765  0.230973   \n",
       "...        ...       ...       ...  ...       ...        ...       ...   \n",
       "4184  0.005951  0.303051 -0.060433  ...  0.999487   0.607368  0.749175   \n",
       "4185  0.333073  0.347843 -0.034620  ...  0.991617   2.392191  0.403972   \n",
       "4186  0.270593  0.506838 -0.128632  ...  0.998685   1.155074  0.648774   \n",
       "4187  0.073176  0.410118 -0.056148  ...  0.999119   0.628868  0.756240   \n",
       "4188  0.009041  0.233875 -0.025062  ...  0.999364   0.327712  0.840301   \n",
       "\n",
       "      Contrast2   Energy3     Corr3  Diss_sim3   Contrast3   Energy4  Target  \n",
       "0     98.646275  0.015894  0.829719  11.224156  208.034439  0.107351       0  \n",
       "1     29.967895  0.021490  0.947064   6.401126   70.011755  0.107569       0  \n",
       "2     30.323549  0.023645  0.897296   6.670947   74.221170  0.124114       0  \n",
       "3     54.709969  0.020782  0.828209   8.687137  125.530404  0.123403       0  \n",
       "4     28.129354  0.023903  0.951120   6.041322   64.345599  0.113598       0  \n",
       "...         ...       ...       ...        ...         ...       ...     ...  \n",
       "4184   1.201333  0.137047  0.998722   0.995182    2.996365  0.203269       5  \n",
       "4185  14.914763  0.045681  0.980559   3.579858   34.599094  0.129531       5  \n",
       "4186   4.162545  0.132761  0.996455   1.920197   11.243822  0.181156       5  \n",
       "4187   1.758523  0.132519  0.997725   1.036403    4.543673  0.201567       5  \n",
       "4188   0.369280  0.194667  0.998609   0.546002    0.808338  0.235993       5  \n",
       "\n",
       "[4189 rows x 272 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop([\"Unnamed: 0\"],axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12e091dd-a39e-42ae-aee8-14c4c105564a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Target\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ebb3476-2e23-47fe-918e-5c9250ccb1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (4189, 271)\n",
      "Reduced shape: (4189, 17)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df.drop(columns=['Target'])\n",
    "y = df['Target']\n",
    "# X = your feature matrix (excluding target column)\n",
    "# Step 1: Scale the data (very important for PCA!)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 2: Apply PCA\n",
    "pca = PCA(n_components=0.95)  # Retain 95% of variance (OR set a fixed number like 50)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(\"Original shape:\", X.shape)\n",
    "print(\"Reduced shape:\", X_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c78ac9f-8a95-46ef-ad97-4c1886991bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the SVM model\n",
    "model = SVC(kernel='rbf', C=1.0, gamma='scale',probability=True)  # You can change kernel to 'linear', 'poly', etc.\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62098287-367c-4b7b-b0c7-e0e1606de47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ mean Average Precision (mAP): 0.9887228851731219\n"
     ]
    }
   ],
   "source": [
    "y_probs = model.predict_proba(X_test)\n",
    "\n",
    "# 2. Convert y_test to binary format for average_precision_score\n",
    "y_test_bin = label_binarize(y_test, classes=sorted(list(set(y))))\n",
    "\n",
    "# 3. Compute mean Average Precision\n",
    "mAP = average_precision_score(y_test_bin, y_probs, average=\"macro\")\n",
    "print(\"\\n✅ mean Average Precision (mAP):\", mAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d220e93-b13b-4d81-9cf3-1f9991dd1e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 4, 0, 2, 0, 0, 5, 1, 2, 5, 5, 2, 1, 2, 2, 0, 1, 0, 4, 2, 5,\n",
       "       0, 0, 4, 2, 2, 4, 1, 2, 1, 2, 1, 3, 1, 2, 1, 1, 5, 4, 2, 3, 2, 5,\n",
       "       4, 1, 5, 2, 1, 4, 5, 1, 5, 0, 1, 0, 2, 4, 0, 5, 1, 5, 4, 1, 4, 2,\n",
       "       5, 5, 1, 1, 5, 2, 0, 4, 4, 4, 0, 2, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "       4, 2, 2, 0, 5, 0, 5, 1, 0, 4, 4, 4, 1, 2, 1, 5, 3, 2, 3, 1, 0, 3,\n",
       "       0, 2, 4, 1, 2, 4, 5, 0, 2, 2, 5, 0, 1, 3, 4, 2, 0, 1, 1, 1, 2, 5,\n",
       "       1, 1, 4, 2, 3, 1, 2, 1, 0, 4, 5, 3, 3, 2, 0, 5, 2, 4, 3, 2, 1, 1,\n",
       "       2, 0, 2, 0, 2, 2, 2, 4, 5, 3, 2, 1, 2, 2, 1, 0, 1, 2, 0, 4, 1, 2,\n",
       "       4, 1, 1, 4, 2, 5, 0, 0, 2, 3, 2, 2, 4, 5, 0, 4, 1, 3, 0, 4, 2, 4,\n",
       "       4, 5, 2, 4, 1, 5, 1, 0, 1, 2, 3, 2, 4, 0, 2, 0, 5, 3, 2, 1, 2, 3,\n",
       "       5, 2, 2, 1, 1, 4, 1, 0, 0, 1, 5, 0, 2, 1, 4, 3, 1, 2, 1, 3, 3, 2,\n",
       "       1, 1, 0, 1, 5, 0, 3, 1, 0, 2, 4, 4, 4, 0, 1, 1, 1, 5, 2, 5, 1, 4,\n",
       "       1, 1, 3, 0, 1, 4, 2, 2, 0, 0, 0, 1, 4, 3, 5, 2, 3, 1, 0, 4, 1, 4,\n",
       "       2, 0, 1, 1, 0, 3, 2, 5, 1, 3, 5, 0, 2, 5, 5, 5, 0, 3, 1, 0, 2, 5,\n",
       "       3, 4, 4, 1, 5, 0, 0, 1, 2, 0, 2, 2, 0, 2, 5, 2, 0, 0, 3, 1, 3, 2,\n",
       "       1, 2, 0, 0, 0, 0, 2, 3, 3, 5, 0, 2, 2, 0, 1, 5, 5, 2, 2, 2, 1, 2,\n",
       "       3, 2, 0, 5, 0, 4, 3, 3, 1, 4, 4, 1, 5, 1, 1, 2, 2, 1, 4, 1, 4, 5,\n",
       "       1, 5, 1, 3, 5, 1, 1, 4, 5, 0, 2, 5, 2, 2, 0, 1, 3, 2, 4, 1, 0, 4,\n",
       "       5, 1, 2, 0, 5, 5, 2, 4, 5, 4, 1, 1, 2, 5, 2, 0, 0, 1, 0, 4, 1, 1,\n",
       "       1, 0, 1, 1, 0, 4, 2, 1, 2, 1, 2, 0, 5, 1, 2, 2, 2, 5, 3, 1, 0, 0,\n",
       "       1, 1, 1, 1, 2, 5, 5, 4, 4, 4, 1, 0, 1, 4, 1, 2, 1, 5, 1, 2, 2, 5,\n",
       "       1, 2, 1, 4, 0, 2, 0, 2, 5, 0, 2, 4, 3, 0, 2, 1, 4, 1, 2, 1, 0, 4,\n",
       "       4, 1, 1, 0, 1, 3, 1, 0, 2, 3, 2, 4, 0, 4, 0, 2, 0, 2, 2, 5, 0, 0,\n",
       "       2, 2, 3, 0, 4, 0, 2, 4, 2, 3, 1, 0, 2, 5, 4, 1, 1, 2, 1, 5, 3, 0,\n",
       "       1, 0, 0, 5, 4, 5, 1, 2, 4, 0, 2, 0, 0, 1, 4, 3, 2, 5, 1, 3, 0, 1,\n",
       "       4, 1, 1, 1, 0, 2, 5, 0, 2, 0, 2, 2, 2, 1, 4, 1, 5, 2, 2, 1, 2, 2,\n",
       "       5, 3, 0, 4, 0, 2, 0, 4, 2, 1, 4, 5, 2, 2, 3, 1, 1, 1, 1, 1, 0, 4,\n",
       "       4, 0, 5, 2, 3, 4, 5, 4, 2, 5, 0, 2, 2, 0, 5, 5, 0, 2, 0, 2, 4, 0,\n",
       "       5, 3, 2, 5, 4, 1, 5, 3, 5, 5, 5, 4, 5, 2, 1, 0, 1, 5, 2, 0, 4, 0,\n",
       "       5, 5, 1, 2, 4, 2, 0, 5, 1, 1, 1, 3, 1, 5, 2, 4, 3, 5, 1, 2, 2, 1,\n",
       "       4, 0, 0, 1, 5, 1, 2, 5, 4, 4, 1, 1, 2, 1, 2, 0, 2, 4, 1, 2, 5, 0,\n",
       "       4, 0, 1, 2, 1, 0, 2, 4, 0, 2, 0, 0, 1, 0, 4, 1, 5, 5, 2, 2, 1, 2,\n",
       "       3, 0, 0, 1, 0, 1, 2, 1, 4, 0, 0, 1, 1, 0, 4, 0, 0, 1, 2, 5, 1, 1,\n",
       "       5, 5, 1, 4, 4, 1, 3, 3, 0, 0, 2, 5, 0, 4, 5, 1, 3, 4, 2, 1, 4, 3,\n",
       "       1, 5, 2, 5, 1, 0, 5, 4, 0, 1, 4, 2, 1, 2, 5, 0, 0, 0, 3, 2, 5, 1,\n",
       "       1, 5, 4, 1, 2, 4, 2, 4, 0, 5, 4, 4, 4, 2, 0, 2, 1, 2, 1, 0, 2, 2,\n",
       "       3, 1, 2, 1, 0, 2, 1, 2, 2, 4, 5, 1, 1, 0, 4, 1, 2, 1, 3, 4, 1, 0,\n",
       "       5, 3, 2, 0, 1, 1, 4, 5, 1, 0, 2, 1, 5, 4, 1, 3, 1, 2, 1, 5, 0, 3,\n",
       "       1, 1], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6485a0c6-85c9-4839-848c-6bdda94ffd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9618138424821002\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.97       159\n",
      "           1       0.94      0.98      0.96       195\n",
      "           2       0.99      0.97      0.98       189\n",
      "           3       0.98      0.97      0.98        64\n",
      "           4       0.93      0.97      0.95       113\n",
      "           5       0.95      0.92      0.93       118\n",
      "\n",
      "    accuracy                           0.96       838\n",
      "   macro avg       0.96      0.96      0.96       838\n",
      "weighted avg       0.96      0.96      0.96       838\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27007e5f-78e7-4ab8-9886-600bd32e8d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8.36382332e+00,  1.14751343e+01,  1.79054123e+01, ...,\n",
       "        -8.02834374e-01,  1.77912800e+00, -6.90334567e-01],\n",
       "       [ 9.37323639e+00, -1.54438929e+01,  4.91965882e+00, ...,\n",
       "        -8.70428190e-01, -4.35706322e-01, -1.11580412e+00],\n",
       "       [ 1.05674802e+01,  9.84883849e+00,  8.98530078e+00, ...,\n",
       "        -1.84929988e+00, -2.94073606e-01,  1.78548445e-01],\n",
       "       ...,\n",
       "       [-2.05893726e+00, -2.67469359e+00, -5.24829795e+00, ...,\n",
       "         2.48009891e-01,  8.21424068e-01,  2.31560608e+00],\n",
       "       [ 1.19061140e+00, -2.36181875e+00, -8.90645148e+00, ...,\n",
       "         4.17122162e+00,  1.35284440e+00, -9.32431816e-03],\n",
       "       [-3.83302947e+00, -2.86788692e+00, -6.52477131e+00, ...,\n",
       "         5.44445063e-01, -1.27384093e+00, -7.23246944e-03]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7a84bfe-c4bc-495b-bd69-5c28eebfaba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_model.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save each component\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "joblib.dump(pca, 'pca.pkl')\n",
    "joblib.dump(model, 'svm_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fefae15a-cf77-4d19-8ff2-e4124eba4f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saiet\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 17 features, but StandardScaler is expecting 271 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msvm_model.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Apply same preprocessing\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m X_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[0;32m      9\u001b[0m X_pca \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mtransform(X_scaled)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Predict\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1045\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1042\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1044\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m-> 1045\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m   1046\u001b[0m     X,\n\u001b[0;32m   1047\u001b[0m     reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1048\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1049\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m   1050\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mFLOAT_DTYPES,\n\u001b[0;32m   1051\u001b[0m     force_writeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1052\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1053\u001b[0m )\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1056\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:654\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 654\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39mreset)\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:443\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    446\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 17 features, but StandardScaler is expecting 271 features as input."
     ]
    }
   ],
   "source": [
    "#note to future self, this is how ur supposed to transform ur input as well\n",
    "\n",
    "scaler = joblib.load(\"scaler.pkl\")\n",
    "pca = joblib.load(\"pca.pkl\")\n",
    "model = joblib.load(\"svm_model.pkl\")\n",
    "\n",
    "# Apply same preprocessing\n",
    "X_scaled = scaler.transform(X_test)\n",
    "X_pca = pca.transform(X_scaled)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_pca)\n",
    "print(\"Predictions:\", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011dd948-11cb-4e46-88d3-d183b940420e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
