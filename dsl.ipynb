{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3edcc474-bc37-4643-8d36-b64aba308b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ultralytics.nn.modules.block.C2f'> <class 'ultralytics.nn.modules.block.C2f_DWRB'> <class 'ultralytics.nn.modules.block.SADown'> <class 'ultralytics.nn.modules.block.LASPPF'>\n"
     ]
    }
   ],
   "source": [
    "from ultralytics.nn.modules import C2f, C2f_DWRB, SADown, LASPPF\n",
    "print(C2f, C2f_DWRB, SADown, LASPPF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38affdd4-8582-4712-8343-bcd96d84570c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class YOLO in module ultralytics.models.yolo.model:\n",
      "\n",
      "class YOLO(ultralytics.engine.model.Model)\n",
      " |  YOLO(model='yolo11n.pt', task=None, verbose=False)\n",
      " |\n",
      " |  YOLO (You Only Look Once) object detection model.\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      YOLO\n",
      " |      ultralytics.engine.model.Model\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, model='yolo11n.pt', task=None, verbose=False)\n",
      " |      Initialize YOLO model, switching to YOLOWorld if model filename contains '-world'.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |\n",
      " |  task_map\n",
      " |      Map head to model, trainer, validator, and predictor classes.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __annotations__ = {}\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ultralytics.engine.model.Model:\n",
      " |\n",
      " |  __call__(self, source: Union[str, pathlib.Path, int, PIL.Image.Image, list, tuple, numpy.ndarray, torch.Tensor] = None, stream: bool = False, **kwargs: Any) -> list\n",
      " |      Alias for the predict method, enabling the model instance to be callable for predictions.\n",
      " |\n",
      " |      This method simplifies the process of making predictions by allowing the model instance to be called\n",
      " |      directly with the required arguments.\n",
      " |\n",
      " |      Args:\n",
      " |          source (str | Path | int | PIL.Image | np.ndarray | torch.Tensor | List | Tuple): The source of\n",
      " |              the image(s) to make predictions on. Can be a file path, URL, PIL image, numpy array, PyTorch\n",
      " |              tensor, or a list/tuple of these.\n",
      " |          stream (bool): If True, treat the input source as a continuous stream for predictions.\n",
      " |          **kwargs: Additional keyword arguments to configure the prediction process.\n",
      " |\n",
      " |      Returns:\n",
      " |          (List[ultralytics.engine.results.Results]): A list of prediction results, each encapsulated in a\n",
      " |              Results object.\n",
      " |\n",
      " |      Examples:\n",
      " |          >>> model = YOLO(\"yolo11n.pt\")\n",
      " |          >>> results = model(\"https://ultralytics.com/images/bus.jpg\")\n",
      " |          >>> for r in results:\n",
      " |          ...     print(f\"Detected {len(r)} objects in image\")\n",
      " |\n",
      " |  __getattr__(self, name)\n",
      " |      Enables accessing model attributes directly through the Model class.\n",
      " |\n",
      " |      This method provides a way to access attributes of the underlying model directly through the Model class\n",
      " |      instance. It first checks if the requested attribute is 'model', in which case it returns the model from\n",
      " |      the module dictionary. Otherwise, it delegates the attribute lookup to the underlying model.\n",
      " |\n",
      " |      Args:\n",
      " |          name (str): The name of the attribute to retrieve.\n",
      " |\n",
      " |      Returns:\n",
      " |          (Any): The requested attribute value.\n",
      " |\n",
      " |      Raises:\n",
      " |          AttributeError: If the requested attribute does not exist in the model.\n",
      " |\n",
      " |      Examples:\n",
      " |          >>> model = YOLO(\"yolo11n.pt\")\n",
      " |          >>> print(model.stride)\n",
      " |          >>> print(model.task)\n",
      " |\n",
      " |  add_callback(self, event: str, func) -> None\n",
      " |      Adds a callback function for a specified event.\n",
      " |\n",
      " |      This method allows registering custom callback functions that are triggered on specific events during\n",
      " |      model operations such as training or inference. Callbacks provide a way to extend and customize the\n",
      " |      behavior of the model at various stages of its lifecycle.\n",
      " |\n",
      " |      Args:\n",
      " |          event (str): The name of the event to attach the callback to. Must be a valid event name recognized\n",
      " |              by the Ultralytics framework.\n",
      " |          func (Callable): The callback function to be registered. This function will be called when the\n",
      " |              specified event occurs.\n",
      " |\n",
      " |      Raises:\n",
      " |          ValueError: If the event name is not recognized or is invalid.\n",
      " |\n",
      " |      Examples:\n",
      " |          >>> def on_train_start(trainer):\n",
      " |          ...     print(\"Training is starting!\")\n",
      " |          >>> model = YOLO(\"yolo11n.pt\")\n",
      " |          >>> model.add_callback(\"on_train_start\", on_train_start)\n",
      " |          >>> model.train(data=\"coco8.yaml\", epochs=1)\n",
      " |\n",
      " |  benchmark(self, **kwargs: Any)\n",
      " |      Benchmarks the model across various export formats to evaluate performance.\n",
      " |\n",
      " |      This method assesses the model's performance in different export formats, such as ONNX, TorchScript, etc.\n",
      " |      It uses the 'benchmark' function from the ultralytics.utils.benchmarks module. The benchmarking is\n",
      " |      configured using a combination of default configuration values, model-specific arguments, method-specific\n",
      " |      defaults, and any additional user-provided keyword arguments.\n",
      " |\n",
      " |      Args:\n",
      " |          **kwargs: Arbitrary keyword arguments to customize the benchmarking process. These are combined with\n",
      " |              default configurations, model-specific arguments, and method defaults. Common options include:\n",
      " |              - data (str): Path to the dataset for benchmarking.\n",
      " |              - imgsz (int | List[int]): Image size for benchmarking.\n",
      " |              - half (bool): Whether to use half-precision (FP16) mode.\n",
      " |              - int8 (bool): Whether to use int8 precision mode.\n",
      " |              - device (str): Device to run the benchmark on (e.g., 'cpu', 'cuda').\n",
      " |              - verbose (bool): Whether to print detailed benchmark information.\n",
      " |              - format (str): Export format name for specific benchmarking\n",
      " |\n",
      " |      Returns:\n",
      " |          (Dict): A dictionary containing the results of the benchmarking process, including metrics for\n",
      " |              different export formats.\n",
      " |\n",
      " |      Raises:\n",
      " |          AssertionError: If the model is not a PyTorch model.\n",
      " |\n",
      " |      Examples:\n",
      " |          >>> model = YOLO(\"yolo11n.pt\")\n",
      " |          >>> results = model.benchmark(data=\"coco8.yaml\", imgsz=640, half=True)\n",
      " |          >>> print(results)\n",
      " |\n",
      " |  clear_callback(self, event: str) -> None\n",
      " |      Clears all callback functions registered for a specified event.\n",
      " |\n",
      " |      This method removes all custom and default callback functions associated with the given event.\n",
      " |      It resets the callback list for the specified event to an empty list, effectively removing all\n",
      " |      registered callbacks for that event.\n",
      " |\n",
      " |      Args:\n",
      " |          event (str): The name of the event for which to clear the callbacks. This should be a valid event name\n",
      " |              recognized by the Ultralytics callback system.\n",
      " |\n",
      " |      Examples:\n",
      " |          >>> model = YOLO(\"yolo11n.pt\")\n",
      " |          >>> model.add_callback(\"on_train_start\", lambda: print(\"Training started\"))\n",
      " |          >>> model.clear_callback(\"on_train_start\")\n",
      " |          >>> # All callbacks for 'on_train_start' are now removed\n",
      " |\n",
      " |      Notes:\n",
      " |          - This method affects both custom callbacks added by the user and default callbacks\n",
      " |            provided by the Ultralytics framework.\n",
      " |          - After calling this method, no callbacks will be executed for the specified event\n",
      " |            until new ones are added.\n",
      " |          - Use with caution as it removes all callbacks, including essential ones that might\n",
      " |            be required for proper functioning of certain operations.\n",
      " |\n",
      " |  embed(self, source: Union[str, pathlib.Path, int, list, tuple, numpy.ndarray, torch.Tensor] = None, stream: bool = False, **kwargs: Any) -> list\n",
      " |      Generates image embeddings based on the provided source.\n",
      " |\n",
      " |      This method is a wrapper around the 'predict()' method, focusing on generating embeddings from an image\n",
      " |      source. It allows customization of the embedding process through various keyword arguments.\n",
      " |\n",
      " |      Args:\n",
      " |          source (str | Path | int | List | Tuple | np.ndarray | torch.Tensor): The source of the image for\n",
      " |              generating embeddings. Can be a file path, URL, PIL image, numpy array, etc.\n",
      " |          stream (bool): If True, predictions are streamed.\n",
      " |          **kwargs: Additional keyword arguments for configuring the embedding process.\n",
      " |\n",
      " |      Returns:\n",
      " |          (List[torch.Tensor]): A list containing the image embeddings.\n",
      " |\n",
      " |      Raises:\n",
      " |          AssertionError: If the model is not a PyTorch model.\n",
      " |\n",
      " |      Examples:\n",
      " |          >>> model = YOLO(\"yolo11n.pt\")\n",
      " |          >>> image = \"https://ultralytics.com/images/bus.jpg\"\n",
      " |          >>> embeddings = model.embed(image)\n",
      " |          >>> print(embeddings[0].shape)\n",
      " |\n",
      " |  eval(self)\n",
      " |      Sets the model to evaluation mode.\n",
      " |\n",
      " |      This method changes the model's mode to evaluation, which affects layers like dropout and batch normalization\n",
      " |      that behave differently during training and evaluation.\n",
      " |\n",
      " |      Returns:\n",
      " |          (Model): The model instance with evaluation mode set.\n",
      " |\n",
      " |      Examples:\n",
      " |          >> model = YOLO(\"yolo11n.pt\")\n",
      " |          >> model.eval()\n",
      " |\n",
      " |  export(self, **kwargs: Any) -> str\n",
      " |      Exports the model to a different format suitable for deployment.\n",
      " |\n",
      " |      This method facilitates the export of the model to various formats (e.g., ONNX, TorchScript) for deployment\n",
      " |      purposes. It uses the 'Exporter' class for the export process, combining model-specific overrides, method\n",
      " |      defaults, and any additional arguments provided.\n",
      " |\n",
      " |      Args:\n",
      " |          **kwargs: Arbitrary keyword arguments to customize the export process. These are combined with\n",
      " |              the model's overrides and method defaults. Common arguments include:\n",
      " |              format (str): Export format (e.g., 'onnx', 'engine', 'coreml').\n",
      " |              half (bool): Export model in half-precision.\n",
      " |              int8 (bool): Export model in int8 precision.\n",
      " |              device (str): Device to run the export on.\n",
      " |              workspace (int): Maximum memory workspace size for TensorRT engines.\n",
      " |              nms (bool): Add Non-Maximum Suppression (NMS) module to model.\n",
      " |              simplify (bool): Simplify ONNX model.\n",
      " |\n",
      " |      Returns:\n",
      " |          (str): The path to the exported model file.\n",
      " |\n",
      " |      Raises:\n",
      " |          AssertionError: If the model is not a PyTorch model.\n",
      " |          ValueError: If an unsupported export format is specified.\n",
      " |          RuntimeError: If the export process fails due to errors.\n",
      " |\n",
      " |      Examples:\n",
      " |          >>> model = YOLO(\"yolo11n.pt\")\n",
      " |          >>> model.export(format=\"onnx\", dynamic=True, simplify=True)\n",
      " |          'path/to/exported/model.onnx'\n",
      " |\n",
      " |  fuse(self)\n",
      " |      Fuses Conv2d and BatchNorm2d layers in the model for optimized inference.\n",
      " |\n",
      " |      This method iterates through the model's modules and fuses consecutive Conv2d and BatchNorm2d layers\n",
      " |      into a single layer. This fusion can significantly improve inference speed by reducing the number of\n",
      " |      operations and memory accesses required during forward passes.\n",
      " |\n",
      " |      The fusion process typically involves folding the BatchNorm2d parameters (mean, variance, weight, and\n",
      " |      bias) into the preceding Conv2d layer's weights and biases. This results in a single Conv2d layer that\n",
      " |      performs both convolution and normalization in one step.\n",
      " |\n",
      " |      Raises:\n",
      " |          TypeError: If the model is not a PyTorch torch.nn.Module.\n",
      " |\n",
      " |      Examples:\n",
      " |          >>> model = Model(\"yolo11n.pt\")\n",
      " |          >>> model.fuse()\n",
      " |          >>> # Model is now fused and ready for optimized inference\n",
      " |\n",
      " |  info(self, detailed: bool = False, verbose: bool = True)\n",
      " |      Logs or returns model information.\n",
      " |\n",
      " |      This method provides an overview or detailed information about the model, depending on the arguments\n",
      " |      passed. It can control the verbosity of the output and return the information as a list.\n",
      " |\n",
      " |      Args:\n",
      " |          detailed (bool): If True, shows detailed information about the model layers and parameters.\n",
      " |          verbose (bool): If True, prints the information. If False, returns the information as a list.\n",
      " |\n",
      " |      Returns:\n",
      " |          (List[str]): A list of strings containing various types of information about the model, including\n",
      " |              model summary, layer details, and parameter counts. Empty if verbose is True.\n",
      " |\n",
      " |      Raises:\n",
      " |          TypeError: If the model is not a PyTorch model.\n",
      " |\n",
      " |      Examples:\n",
      " |          >>> model = Model(\"yolo11n.pt\")\n",
      " |          >>> model.info()  # Prints model summary\n",
      " |          >>> info_list = model.info(detailed=True, verbose=False)  # Returns detailed info as a list\n",
      " |\n",
      " |  load(self, weights: Union[str, pathlib.Path] = 'yolo11n.pt') -> 'Model'\n",
      " |      Loads parameters from the specified weights file into the model.\n",
      " |\n",
      " |      This method supports loading weights from a file or directly from a weights object. It matches parameters by\n",
      " |      name and shape and transfers them to the model.\n",
      " |\n",
      " |      Args:\n",
      " |          weights (Union[str, Path]): Path to the weights file or a weights object.\n",
      " |\n",
      " |      Returns:\n",
      " |          (Model): The instance of the class with loaded weights.\n",
      " |\n",
      " |      Raises:\n",
      " |          AssertionError: If the model is not a PyTorch model.\n",
      " |\n",
      " |      Examples:\n",
      " |          >>> model = Model()\n",
      " |          >>> model.load(\"yolo11n.pt\")\n",
      " |          >>> model.load(Path(\"path/to/weights.pt\"))\n",
      " |\n",
      " |  predict(self, source: Union[str, pathlib.Path, int, PIL.Image.Image, list, tuple, numpy.ndarray, torch.Tensor] = None, stream: bool = False, predictor=None, **kwargs: Any) -> List[ultralytics.engine.results.Results]\n",
      " |      Performs predictions on the given image source using the YOLO model.\n",
      " |\n",
      " |      This method facilitates the prediction process, allowing various configurations through keyword arguments.\n",
      " |      It supports predictions with custom predictors or the default predictor method. The method handles different\n",
      " |      types of image sources and can operate in a streaming mode.\n",
      " |\n",
      " |      Args:\n",
      " |          source (str | Path | int | PIL.Image | np.ndarray | torch.Tensor | List | Tuple): The source\n",
      " |              of the image(s) to make predictions on. Accepts various types including file paths, URLs, PIL\n",
      " |              images, numpy arrays, and torch tensors.\n",
      " |          stream (bool): If True, treats the input source as a continuous stream for predictions.\n",
      " |          predictor (BasePredictor | None): An instance of a custom predictor class for making predictions.\n",
      " |              If None, the method uses a default predictor.\n",
      " |          **kwargs: Additional keyword arguments for configuring the prediction process.\n",
      " |\n",
      " |      Returns:\n",
      " |          (List[ultralytics.engine.results.Results]): A list of prediction results, each encapsulated in a\n",
      " |              Results object.\n",
      " |\n",
      " |      Examples:\n",
      " |          >>> model = YOLO(\"yolo11n.pt\")\n",
      " |          >>> results = model.predict(source=\"path/to/image.jpg\", conf=0.25)\n",
      " |          >>> for r in results:\n",
      " |          ...     print(r.boxes.data)  # print detection bounding boxes\n",
      " |\n",
      " |      Notes:\n",
      " |          - If 'source' is not provided, it defaults to the ASSETS constant with a warning.\n",
      " |          - The method sets up a new predictor if not already present and updates its arguments with each call.\n",
      " |          - For SAM-type models, 'prompts' can be passed as a keyword argument.\n",
      " |\n",
      " |  reset_callbacks(self) -> None\n",
      " |      Resets all callbacks to their default functions.\n",
      " |\n",
      " |      This method reinstates the default callback functions for all events, removing any custom callbacks that were\n",
      " |      previously added. It iterates through all default callback events and replaces the current callbacks with the\n",
      " |      default ones.\n",
      " |\n",
      " |      The default callbacks are defined in the 'callbacks.default_callbacks' dictionary, which contains predefined\n",
      " |      functions for various events in the model's lifecycle, such as on_train_start, on_epoch_end, etc.\n",
      " |\n",
      " |      This method is useful when you want to revert to the original set of callbacks after making custom\n",
      " |      modifications, ensuring consistent behavior across different runs or experiments.\n",
      " |\n",
      " |      Examples:\n",
      " |          >>> model = YOLO(\"yolo11n.pt\")\n",
      " |          >>> model.add_callback(\"on_train_start\", custom_function)\n",
      " |          >>> model.reset_callbacks()\n",
      " |          # All callbacks are now reset to their default functions\n",
      " |\n",
      " |  reset_weights(self) -> 'Model'\n",
      " |      Resets the model's weights to their initial state.\n",
      " |\n",
      " |      This method iterates through all modules in the model and resets their parameters if they have a\n",
      " |      'reset_parameters' method. It also ensures that all parameters have 'requires_grad' set to True,\n",
      " |      enabling them to be updated during training.\n",
      " |\n",
      " |      Returns:\n",
      " |          (Model): The instance of the class with reset weights.\n",
      " |\n",
      " |      Raises:\n",
      " |          AssertionError: If the model is not a PyTorch model.\n",
      " |\n",
      " |      Examples:\n",
      " |          >>> model = Model(\"yolo11n.pt\")\n",
      " |          >>> model.reset_weights()\n",
      " |\n",
      " |  save(self, filename: Union[str, pathlib.Path] = 'saved_model.pt') -> None\n",
      " |      Saves the current model state to a file.\n",
      " |\n",
      " |      This method exports the model's checkpoint (ckpt) to the specified filename. It includes metadata such as\n",
      " |      the date, Ultralytics version, license information, and a link to the documentation.\n",
      " |\n",
      " |      Args:\n",
      " |          filename (Union[str, Path]): The name of the file to save the model to.\n",
      " |\n",
      " |      Raises:\n",
      " |          AssertionError: If the model is not a PyTorch model.\n",
      " |\n",
      " |      Examples:\n",
      " |          >>> model = Model(\"yolo11n.pt\")\n",
      " |          >>> model.save(\"my_model.pt\")\n",
      " |\n",
      " |  track(self, source: Union[str, pathlib.Path, int, list, tuple, numpy.ndarray, torch.Tensor] = None, stream: bool = False, persist: bool = False, **kwargs: Any) -> List[ultralytics.engine.results.Results]\n",
      " |      Conducts object tracking on the specified input source using the registered trackers.\n",
      " |\n",
      " |      This method performs object tracking using the model's predictors and optionally registered trackers. It handles\n",
      " |      various input sources such as file paths or video streams, and supports customization through keyword arguments.\n",
      " |      The method registers trackers if not already present and can persist them between calls.\n",
      " |\n",
      " |      Args:\n",
      " |          source (Union[str, Path, int, List, Tuple, np.ndarray, torch.Tensor], optional): Input source for object\n",
      " |              tracking. Can be a file path, URL, or video stream.\n",
      " |          stream (bool): If True, treats the input source as a continuous video stream. Defaults to False.\n",
      " |          persist (bool): If True, persists trackers between different calls to this method. Defaults to False.\n",
      " |          **kwargs: Additional keyword arguments for configuring the tracking process.\n",
      " |\n",
      " |      Returns:\n",
      " |          (List[ultralytics.engine.results.Results]): A list of tracking results, each a Results object.\n",
      " |\n",
      " |      Raises:\n",
      " |          AttributeError: If the predictor does not have registered trackers.\n",
      " |\n",
      " |      Examples:\n",
      " |          >>> model = YOLO(\"yolo11n.pt\")\n",
      " |          >>> results = model.track(source=\"path/to/video.mp4\", show=True)\n",
      " |          >>> for r in results:\n",
      " |          ...     print(r.boxes.id)  # print tracking IDs\n",
      " |\n",
      " |      Notes:\n",
      " |          - This method sets a default confidence threshold of 0.1 for ByteTrack-based tracking.\n",
      " |          - The tracking mode is explicitly set in the keyword arguments.\n",
      " |          - Batch size is set to 1 for tracking in videos.\n",
      " |\n",
      " |  train(self, trainer=None, **kwargs: Any)\n",
      " |      Trains the model using the specified dataset and training configuration.\n",
      " |\n",
      " |      This method facilitates model training with a range of customizable settings. It supports training with a\n",
      " |      custom trainer or the default training approach. The method handles scenarios such as resuming training\n",
      " |      from a checkpoint, integrating with Ultralytics HUB, and updating model and configuration after training.\n",
      " |\n",
      " |      When using Ultralytics HUB, if the session has a loaded model, the method prioritizes HUB training\n",
      " |      arguments and warns if local arguments are provided. It checks for pip updates and combines default\n",
      " |      configurations, method-specific defaults, and user-provided arguments to configure the training process.\n",
      " |\n",
      " |      Args:\n",
      " |          trainer (BaseTrainer | None): Custom trainer instance for model training. If None, uses default.\n",
      " |          **kwargs: Arbitrary keyword arguments for training configuration. Common options include:\n",
      " |              data (str): Path to dataset configuration file.\n",
      " |              epochs (int): Number of training epochs.\n",
      " |              batch_size (int): Batch size for training.\n",
      " |              imgsz (int): Input image size.\n",
      " |              device (str): Device to run training on (e.g., 'cuda', 'cpu').\n",
      " |              workers (int): Number of worker threads for data loading.\n",
      " |              optimizer (str): Optimizer to use for training.\n",
      " |              lr0 (float): Initial learning rate.\n",
      " |              patience (int): Epochs to wait for no observable improvement for early stopping of training.\n",
      " |\n",
      " |      Returns:\n",
      " |          (Dict | None): Training metrics if available and training is successful; otherwise, None.\n",
      " |\n",
      " |      Raises:\n",
      " |          AssertionError: If the model is not a PyTorch model.\n",
      " |          PermissionError: If there is a permission issue with the HUB session.\n",
      " |          ModuleNotFoundError: If the HUB SDK is not installed.\n",
      " |\n",
      " |      Examples:\n",
      " |          >>> model = YOLO(\"yolo11n.pt\")\n",
      " |          >>> results = model.train(data=\"coco8.yaml\", epochs=3)\n",
      " |\n",
      " |  tune(self, use_ray=False, iterations=10, *args: Any, **kwargs: Any)\n",
      " |      Conducts hyperparameter tuning for the model, with an option to use Ray Tune.\n",
      " |\n",
      " |      This method supports two modes of hyperparameter tuning: using Ray Tune or a custom tuning method.\n",
      " |      When Ray Tune is enabled, it leverages the 'run_ray_tune' function from the ultralytics.utils.tuner module.\n",
      " |      Otherwise, it uses the internal 'Tuner' class for tuning. The method combines default, overridden, and\n",
      " |      custom arguments to configure the tuning process.\n",
      " |\n",
      " |      Args:\n",
      " |          use_ray (bool): If True, uses Ray Tune for hyperparameter tuning. Defaults to False.\n",
      " |          iterations (int): The number of tuning iterations to perform. Defaults to 10.\n",
      " |          *args: Variable length argument list for additional arguments.\n",
      " |          **kwargs: Arbitrary keyword arguments. These are combined with the model's overrides and defaults.\n",
      " |\n",
      " |      Returns:\n",
      " |          (Dict): A dictionary containing the results of the hyperparameter search.\n",
      " |\n",
      " |      Raises:\n",
      " |          AssertionError: If the model is not a PyTorch model.\n",
      " |\n",
      " |      Examples:\n",
      " |          >>> model = YOLO(\"yolo11n.pt\")\n",
      " |          >>> results = model.tune(use_ray=True, iterations=20)\n",
      " |          >>> print(results)\n",
      " |\n",
      " |  val(self, validator=None, **kwargs: Any)\n",
      " |      Validates the model using a specified dataset and validation configuration.\n",
      " |\n",
      " |      This method facilitates the model validation process, allowing for customization through various settings. It\n",
      " |      supports validation with a custom validator or the default validation approach. The method combines default\n",
      " |      configurations, method-specific defaults, and user-provided arguments to configure the validation process.\n",
      " |\n",
      " |      Args:\n",
      " |          validator (ultralytics.engine.validator.BaseValidator | None): An instance of a custom validator class for\n",
      " |              validating the model.\n",
      " |          **kwargs: Arbitrary keyword arguments for customizing the validation process.\n",
      " |\n",
      " |      Returns:\n",
      " |          (ultralytics.utils.metrics.DetMetrics): Validation metrics obtained from the validation process.\n",
      " |\n",
      " |      Raises:\n",
      " |          AssertionError: If the model is not a PyTorch model.\n",
      " |\n",
      " |      Examples:\n",
      " |          >>> model = YOLO(\"yolo11n.pt\")\n",
      " |          >>> results = model.val(data=\"coco8.yaml\", imgsz=640)\n",
      " |          >>> print(results.box.map)  # Print mAP50-95\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from ultralytics.engine.model.Model:\n",
      " |\n",
      " |  is_hub_model(model: str) -> bool\n",
      " |      Check if the provided model is an Ultralytics HUB model.\n",
      " |\n",
      " |      This static method determines whether the given model string represents a valid Ultralytics HUB model\n",
      " |      identifier.\n",
      " |\n",
      " |      Args:\n",
      " |          model (str): The model string to check.\n",
      " |\n",
      " |      Returns:\n",
      " |          (bool): True if the model is a valid Ultralytics HUB model, False otherwise.\n",
      " |\n",
      " |      Examples:\n",
      " |          >>> Model.is_hub_model(\"https://hub.ultralytics.com/models/MODEL\")\n",
      " |          True\n",
      " |          >>> Model.is_hub_model(\"yolo11n.pt\")\n",
      " |          False\n",
      " |\n",
      " |  is_triton_model(model: str) -> bool\n",
      " |      Checks if the given model string is a Triton Server URL.\n",
      " |\n",
      " |      This static method determines whether the provided model string represents a valid Triton Server URL by\n",
      " |      parsing its components using urllib.parse.urlsplit().\n",
      " |\n",
      " |      Args:\n",
      " |          model (str): The model string to be checked.\n",
      " |\n",
      " |      Returns:\n",
      " |          (bool): True if the model string is a valid Triton Server URL, False otherwise.\n",
      " |\n",
      " |      Examples:\n",
      " |          >>> Model.is_triton_model(\"http://localhost:8000/v2/models/yolo11n\")\n",
      " |          True\n",
      " |          >>> Model.is_triton_model(\"yolo11n.pt\")\n",
      " |          False\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from ultralytics.engine.model.Model:\n",
      " |\n",
      " |  device\n",
      " |      Retrieves the device on which the model's parameters are allocated.\n",
      " |\n",
      " |      This property determines the device (CPU or GPU) where the model's parameters are currently stored. It is\n",
      " |      applicable only to models that are instances of torch.nn.Module.\n",
      " |\n",
      " |      Returns:\n",
      " |          (torch.device): The device (CPU/GPU) of the model.\n",
      " |\n",
      " |      Raises:\n",
      " |          AttributeError: If the model is not a torch.nn.Module instance.\n",
      " |\n",
      " |      Examples:\n",
      " |          >>> model = YOLO(\"yolo11n.pt\")\n",
      " |          >>> print(model.device)\n",
      " |          device(type='cuda', index=0)  # if CUDA is available\n",
      " |          >>> model = model.to(\"cpu\")\n",
      " |          >>> print(model.device)\n",
      " |          device(type='cpu')\n",
      " |\n",
      " |  names\n",
      " |      Retrieves the class names associated with the loaded model.\n",
      " |\n",
      " |      This property returns the class names if they are defined in the model. It checks the class names for validity\n",
      " |      using the 'check_class_names' function from the ultralytics.nn.autobackend module. If the predictor is not\n",
      " |      initialized, it sets it up before retrieving the names.\n",
      " |\n",
      " |      Returns:\n",
      " |          (Dict[int, str]): A dict of class names associated with the model.\n",
      " |\n",
      " |      Raises:\n",
      " |          AttributeError: If the model or predictor does not have a 'names' attribute.\n",
      " |\n",
      " |      Examples:\n",
      " |          >>> model = YOLO(\"yolo11n.pt\")\n",
      " |          >>> print(model.names)\n",
      " |          {0: 'person', 1: 'bicycle', 2: 'car', ...}\n",
      " |\n",
      " |  transforms\n",
      " |      Retrieves the transformations applied to the input data of the loaded model.\n",
      " |\n",
      " |      This property returns the transformations if they are defined in the model. The transforms\n",
      " |      typically include preprocessing steps like resizing, normalization, and data augmentation\n",
      " |      that are applied to input data before it is fed into the model.\n",
      " |\n",
      " |      Returns:\n",
      " |          (object | None): The transform object of the model if available, otherwise None.\n",
      " |\n",
      " |      Examples:\n",
      " |          >>> model = YOLO(\"yolo11n.pt\")\n",
      " |          >>> transforms = model.transforms\n",
      " |          >>> if transforms:\n",
      " |          ...     print(f\"Model transforms: {transforms}\")\n",
      " |          ... else:\n",
      " |          ...     print(\"No transforms defined for this model.\")\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |\n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |\n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |\n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |\n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      " |      Implement setattr(self, name, value).\n",
      " |\n",
      " |  __setstate__(self, state)\n",
      " |\n",
      " |  add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Add a child module to the current module.\n",
      " |\n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |\n",
      " |      Args:\n",
      " |          name (str): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          module (Module): child module to be added to the module.\n",
      " |\n",
      " |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      " |      Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n",
      " |\n",
      " |      Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`nn-init-doc`).\n",
      " |\n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> @torch.no_grad()\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[1., 1.],\n",
      " |                  [1., 1.]], requires_grad=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[1., 1.],\n",
      " |                  [1., 1.]], requires_grad=True)\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |\n",
      " |  bfloat16(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      " |\n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      " |      Return an iterator over module buffers.\n",
      " |\n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |\n",
      " |      Yields:\n",
      " |          torch.Tensor: module buffer\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for buf in model.buffers():\n",
      " |          >>>     print(type(buf), buf.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |\n",
      " |  children(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Return an iterator over immediate children modules.\n",
      " |\n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |\n",
      " |  compile(self, *args, **kwargs)\n",
      " |      Compile this Module's forward using :func:`torch.compile`.\n",
      " |\n",
      " |      This Module's `__call__` method is compiled and all arguments are passed as-is\n",
      " |      to :func:`torch.compile`.\n",
      " |\n",
      " |      See :func:`torch.compile` for details on the arguments for this function.\n",
      " |\n",
      " |  cpu(self: ~T) -> ~T\n",
      " |      Move all model parameters and buffers to the CPU.\n",
      " |\n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Move all model parameters and buffers to the GPU.\n",
      " |\n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |\n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |\n",
      " |      Args:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |  double(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      " |\n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |  extra_repr(self) -> str\n",
      " |      Set the extra representation of the module.\n",
      " |\n",
      " |      To print customized extra information, you should re-implement\n",
      " |      this method in your own modules. Both single-line and multi-line\n",
      " |      strings are acceptable.\n",
      " |\n",
      " |  float(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      " |\n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |  forward = _forward_unimplemented(self, *input: Any) -> None from torch.nn.modules.module\n",
      " |      Define the computation performed at every call.\n",
      " |\n",
      " |      Should be overridden by all subclasses.\n",
      " |\n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within\n",
      " |          this function, one should call the :class:`Module` instance afterwards\n",
      " |          instead of this since the former takes care of running the\n",
      " |          registered hooks while the latter silently ignores them.\n",
      " |\n",
      " |  get_buffer(self, target: str) -> 'Tensor'\n",
      " |      Return the buffer given by ``target`` if it exists, otherwise throw an error.\n",
      " |\n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |\n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the buffer\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |\n",
      " |      Returns:\n",
      " |          torch.Tensor: The buffer referenced by ``target``\n",
      " |\n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not a\n",
      " |              buffer\n",
      " |\n",
      " |  get_extra_state(self) -> Any\n",
      " |      Return any extra state to include in the module's state_dict.\n",
      " |\n",
      " |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      " |      if you need to store extra state. This function is called when building the\n",
      " |      module's `state_dict()`.\n",
      " |\n",
      " |      Note that extra state should be picklable to ensure working serialization\n",
      " |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      " |      for serializing Tensors; other objects may break backwards compatibility if\n",
      " |      their serialized pickled form changes.\n",
      " |\n",
      " |      Returns:\n",
      " |          object: Any extra state to store in the module's state_dict\n",
      " |\n",
      " |  get_parameter(self, target: str) -> 'Parameter'\n",
      " |      Return the parameter given by ``target`` if it exists, otherwise throw an error.\n",
      " |\n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |\n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the Parameter\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |\n",
      " |      Returns:\n",
      " |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      " |\n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Parameter``\n",
      " |\n",
      " |  get_submodule(self, target: str) -> 'Module'\n",
      " |      Return the submodule given by ``target`` if it exists, otherwise throw an error.\n",
      " |\n",
      " |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      " |      looks like this:\n",
      " |\n",
      " |      .. code-block:: text\n",
      " |\n",
      " |          A(\n",
      " |              (net_b): Module(\n",
      " |                  (net_c): Module(\n",
      " |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      " |                  )\n",
      " |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      " |              )\n",
      " |          )\n",
      " |\n",
      " |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      " |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      " |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      " |\n",
      " |      To check whether or not we have the ``linear`` submodule, we\n",
      " |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      " |      we have the ``conv`` submodule, we would call\n",
      " |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      " |\n",
      " |      The runtime of ``get_submodule`` is bounded by the degree\n",
      " |      of module nesting in ``target``. A query against\n",
      " |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      " |      the number of transitive modules. So, for a simple check to see\n",
      " |      if some submodule exists, ``get_submodule`` should always be\n",
      " |      used.\n",
      " |\n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the submodule\n",
      " |              to look for. (See above example for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |\n",
      " |      Returns:\n",
      " |          torch.nn.Module: The submodule referenced by ``target``\n",
      " |\n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Module``\n",
      " |\n",
      " |  half(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      " |\n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |  ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Move all model parameters and buffers to the IPU.\n",
      " |\n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on IPU while being optimized.\n",
      " |\n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |\n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |  load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)\n",
      " |      Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n",
      " |\n",
      " |      If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |\n",
      " |      .. warning::\n",
      " |          If :attr:`assign` is ``True`` the optimizer must be created after\n",
      " |          the call to :attr:`load_state_dict` unless\n",
      " |          :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.\n",
      " |\n",
      " |      Args:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |          assign (bool, optional): When ``False``, the properties of the tensors\n",
      " |              in the current module are preserved while when ``True``, the\n",
      " |              properties of the Tensors in the state dict are preserved. The only\n",
      " |              exception is the ``requires_grad`` field of :class:`~torch.nn.Parameter`s\n",
      " |              for which the value from the module is preserved.\n",
      " |              Default: ``False``\n",
      " |\n",
      " |      Returns:\n",
      " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      " |              * **missing_keys** is a list of str containing any keys that are expected\n",
      " |                  by this module but missing from the provided ``state_dict``.\n",
      " |              * **unexpected_keys** is a list of str containing the keys that are not\n",
      " |                  expected by this module but present in the provided ``state_dict``.\n",
      " |\n",
      " |      Note:\n",
      " |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      " |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      " |          ``RuntimeError``.\n",
      " |\n",
      " |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Return an iterator over all modules in the network.\n",
      " |\n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |\n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |          ...     print(idx, '->', m)\n",
      " |\n",
      " |          0 -> Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      " |\n",
      " |  mtia(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Move all model parameters and buffers to the MTIA.\n",
      " |\n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on MTIA while being optimized.\n",
      " |\n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |\n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |  named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      " |      Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.\n",
      " |\n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all buffer names.\n",
      " |          recurse (bool, optional): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module. Defaults to True.\n",
      " |          remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\n",
      " |\n",
      " |      Yields:\n",
      " |          (str, torch.Tensor): Tuple containing the name and buffer\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, buf in self.named_buffers():\n",
      " |          >>>     if name in ['running_var']:\n",
      " |          >>>         print(buf.size())\n",
      " |\n",
      " |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      " |      Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n",
      " |\n",
      " |      Yields:\n",
      " |          (str, Module): Tuple containing a name and child module\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |\n",
      " |  named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      " |      Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.\n",
      " |\n",
      " |      Args:\n",
      " |          memo: a memo to store the set of modules already added to the result\n",
      " |          prefix: a prefix that will be added to the name of the module\n",
      " |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      " |              or not\n",
      " |\n",
      " |      Yields:\n",
      " |          (str, Module): Tuple of name and module\n",
      " |\n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |          ...     print(idx, '->', m)\n",
      " |\n",
      " |          0 -> ('', Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      " |\n",
      " |  named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      " |      Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\n",
      " |\n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all parameter names.\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |          remove_duplicate (bool, optional): whether to remove the duplicated\n",
      " |              parameters in the result. Defaults to True.\n",
      " |\n",
      " |      Yields:\n",
      " |          (str, Parameter): Tuple containing the name and parameter\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>     if name in ['bias']:\n",
      " |          >>>         print(param.size())\n",
      " |\n",
      " |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      " |      Return an iterator over module parameters.\n",
      " |\n",
      " |      This is typically passed to an optimizer.\n",
      " |\n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |\n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param), param.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |\n",
      " |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a backward hook on the module.\n",
      " |\n",
      " |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      " |      the behavior of this function will change in future versions.\n",
      " |\n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |\n",
      " |  register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None\n",
      " |      Add a buffer to the module.\n",
      " |\n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the module's state. Buffers, by\n",
      " |      default, are persistent and will be saved alongside parameters. This\n",
      " |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      " |      only difference between a persistent buffer and a non-persistent buffer\n",
      " |      is that the latter will not be a part of this module's\n",
      " |      :attr:`state_dict`.\n",
      " |\n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |\n",
      " |      Args:\n",
      " |          name (str): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      " |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      " |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      " |          persistent (bool): whether the buffer is part of this module's\n",
      " |              :attr:`state_dict`.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |\n",
      " |  register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a forward hook on the module.\n",
      " |\n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |\n",
      " |      If ``with_kwargs`` is ``False`` or not specified, the input contains only\n",
      " |      the positional arguments given to the module. Keyword arguments won't be\n",
      " |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      " |      output. It can modify the input inplace but it will not have effect on\n",
      " |      forward since this is called after :func:`forward` is called. The hook\n",
      " |      should have the following signature::\n",
      " |\n",
      " |          hook(module, args, output) -> None or modified output\n",
      " |\n",
      " |      If ``with_kwargs`` is ``True``, the forward hook will be passed the\n",
      " |      ``kwargs`` given to the forward function and be expected to return the\n",
      " |      output possibly modified. The hook should have the following signature::\n",
      " |\n",
      " |          hook(module, args, kwargs, output) -> None or modified output\n",
      " |\n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |          prepend (bool): If ``True``, the provided ``hook`` will be fired\n",
      " |              before all existing ``forward`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``forward`` hooks on\n",
      " |              this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``forward`` hooks registered with\n",
      " |              :func:`register_module_forward_hook` will fire before all hooks\n",
      " |              registered by this method.\n",
      " |              Default: ``False``\n",
      " |          with_kwargs (bool): If ``True``, the ``hook`` will be passed the\n",
      " |              kwargs given to the forward function.\n",
      " |              Default: ``False``\n",
      " |          always_call (bool): If ``True`` the ``hook`` will be run regardless of\n",
      " |              whether an exception is raised while calling the Module.\n",
      " |              Default: ``False``\n",
      " |\n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |\n",
      " |  register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a forward pre-hook on the module.\n",
      " |\n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |\n",
      " |\n",
      " |      If ``with_kwargs`` is false or not specified, the input contains only\n",
      " |      the positional arguments given to the module. Keyword arguments won't be\n",
      " |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      " |      input. User can either return a tuple or a single modified value in the\n",
      " |      hook. We will wrap the value into a tuple if a single value is returned\n",
      " |      (unless that value is already a tuple). The hook should have the\n",
      " |      following signature::\n",
      " |\n",
      " |          hook(module, args) -> None or modified input\n",
      " |\n",
      " |      If ``with_kwargs`` is true, the forward pre-hook will be passed the\n",
      " |      kwargs given to the forward function. And if the hook modifies the\n",
      " |      input, both the args and kwargs should be returned. The hook should have\n",
      " |      the following signature::\n",
      " |\n",
      " |          hook(module, args, kwargs) -> None or a tuple of modified input and kwargs\n",
      " |\n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``forward_pre`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``forward_pre`` hooks\n",
      " |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``forward_pre`` hooks registered with\n",
      " |              :func:`register_module_forward_pre_hook` will fire before all\n",
      " |              hooks registered by this method.\n",
      " |              Default: ``False``\n",
      " |          with_kwargs (bool): If true, the ``hook`` will be passed the kwargs\n",
      " |              given to the forward function.\n",
      " |              Default: ``False``\n",
      " |\n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |\n",
      " |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a backward hook on the module.\n",
      " |\n",
      " |      The hook will be called every time the gradients with respect to a module\n",
      " |      are computed, i.e. the hook will execute if and only if the gradients with\n",
      " |      respect to module outputs are computed. The hook should have the following\n",
      " |      signature::\n",
      " |\n",
      " |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      " |\n",
      " |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      " |      with respect to the inputs and outputs respectively. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      " |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      " |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      " |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      " |      arguments.\n",
      " |\n",
      " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      " |      of each Tensor returned by the Module's forward function.\n",
      " |\n",
      " |      .. warning ::\n",
      " |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |\n",
      " |      Args:\n",
      " |          hook (Callable): The user-defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``backward`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``backward`` hooks on\n",
      " |              this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``backward`` hooks registered with\n",
      " |              :func:`register_module_full_backward_hook` will fire before\n",
      " |              all hooks registered by this method.\n",
      " |\n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |\n",
      " |  register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a backward pre-hook on the module.\n",
      " |\n",
      " |      The hook will be called every time the gradients for the module are computed.\n",
      " |      The hook should have the following signature::\n",
      " |\n",
      " |          hook(module, grad_output) -> tuple[Tensor] or None\n",
      " |\n",
      " |      The :attr:`grad_output` is a tuple. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the output that will be used in place of :attr:`grad_output` in\n",
      " |      subsequent computations. Entries in :attr:`grad_output` will be ``None`` for\n",
      " |      all non-Tensor arguments.\n",
      " |\n",
      " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      " |      of each Tensor returned by the Module's forward function.\n",
      " |\n",
      " |      .. warning ::\n",
      " |          Modifying inputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |\n",
      " |      Args:\n",
      " |          hook (Callable): The user-defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``backward_pre`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``backward_pre`` hooks\n",
      " |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``backward_pre`` hooks registered with\n",
      " |              :func:`register_module_full_backward_pre_hook` will fire before\n",
      " |              all hooks registered by this method.\n",
      " |\n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |\n",
      " |  register_load_state_dict_post_hook(self, hook)\n",
      " |      Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n",
      " |\n",
      " |      It should have the following signature::\n",
      " |          hook(module, incompatible_keys) -> None\n",
      " |\n",
      " |      The ``module`` argument is the current module that this hook is registered\n",
      " |      on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\n",
      " |      of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\n",
      " |      is a ``list`` of ``str`` containing the missing keys and\n",
      " |      ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n",
      " |\n",
      " |      The given incompatible_keys can be modified inplace if needed.\n",
      " |\n",
      " |      Note that the checks performed when calling :func:`load_state_dict` with\n",
      " |      ``strict=True`` are affected by modifications the hook makes to\n",
      " |      ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\n",
      " |      set of keys will result in an error being thrown when ``strict=True``, and\n",
      " |      clearing out both missing and unexpected keys will avoid an error.\n",
      " |\n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |\n",
      " |  register_load_state_dict_pre_hook(self, hook)\n",
      " |      Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n",
      " |\n",
      " |      It should have the following signature::\n",
      " |          hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950\n",
      " |\n",
      " |      Arguments:\n",
      " |          hook (Callable): Callable hook that will be invoked before\n",
      " |              loading the state dict.\n",
      " |\n",
      " |  register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Alias for :func:`add_module`.\n",
      " |\n",
      " |  register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None\n",
      " |      Add a parameter to the module.\n",
      " |\n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |\n",
      " |      Args:\n",
      " |          name (str): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          param (Parameter or None): parameter to be added to the module. If\n",
      " |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      " |              are ignored. If ``None``, the parameter is **not** included in the\n",
      " |              module's :attr:`state_dict`.\n",
      " |\n",
      " |  register_state_dict_post_hook(self, hook)\n",
      " |      Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n",
      " |\n",
      " |      It should have the following signature::\n",
      " |          hook(module, state_dict, prefix, local_metadata) -> None\n",
      " |\n",
      " |      The registered hooks can modify the ``state_dict`` inplace.\n",
      " |\n",
      " |  register_state_dict_pre_hook(self, hook)\n",
      " |      Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n",
      " |\n",
      " |      It should have the following signature::\n",
      " |          hook(module, prefix, keep_vars) -> None\n",
      " |\n",
      " |      The registered hooks can be used to perform pre-processing before the ``state_dict``\n",
      " |      call is made.\n",
      " |\n",
      " |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      " |      Change if autograd should record operations on parameters in this module.\n",
      " |\n",
      " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      " |      in-place.\n",
      " |\n",
      " |      This method is helpful for freezing part of the module for finetuning\n",
      " |      or training parts of a model individually (e.g., GAN training).\n",
      " |\n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      " |\n",
      " |      Args:\n",
      " |          requires_grad (bool): whether autograd should record operations on\n",
      " |                                parameters in this module. Default: ``True``.\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |  set_extra_state(self, state: Any) -> None\n",
      " |      Set extra state contained in the loaded `state_dict`.\n",
      " |\n",
      " |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      " |      found within the `state_dict`. Implement this function and a corresponding\n",
      " |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      " |      `state_dict`.\n",
      " |\n",
      " |      Args:\n",
      " |          state (dict): Extra state from the `state_dict`\n",
      " |\n",
      " |  set_submodule(self, target: str, module: 'Module') -> None\n",
      " |      Set the submodule given by ``target`` if it exists, otherwise throw an error.\n",
      " |\n",
      " |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      " |      looks like this:\n",
      " |\n",
      " |      .. code-block:: text\n",
      " |\n",
      " |          A(\n",
      " |              (net_b): Module(\n",
      " |                  (net_c): Module(\n",
      " |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      " |                  )\n",
      " |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      " |              )\n",
      " |          )\n",
      " |\n",
      " |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      " |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      " |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      " |\n",
      " |      To overide the ``Conv2d`` with a new submodule ``Linear``, you\n",
      " |      would call\n",
      " |      ``set_submodule(\"net_b.net_c.conv\", nn.Linear(33, 16))``.\n",
      " |\n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the submodule\n",
      " |              to look for. (See above example for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |          module: The module to set the submodule to.\n",
      " |\n",
      " |      Raises:\n",
      " |          ValueError: If the target string is empty\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Module``\n",
      " |\n",
      " |  share_memory(self: ~T) -> ~T\n",
      " |      See :meth:`torch.Tensor.share_memory_`.\n",
      " |\n",
      " |  state_dict(self, *args, destination=None, prefix='', keep_vars=False)\n",
      " |      Return a dictionary containing references to the whole state of the module.\n",
      " |\n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      Parameters and buffers set to ``None`` are not included.\n",
      " |\n",
      " |      .. note::\n",
      " |          The returned object is a shallow copy. It contains references\n",
      " |          to the module's parameters and buffers.\n",
      " |\n",
      " |      .. warning::\n",
      " |          Currently ``state_dict()`` also accepts positional arguments for\n",
      " |          ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n",
      " |          this is being deprecated and keyword arguments will be enforced in\n",
      " |          future releases.\n",
      " |\n",
      " |      .. warning::\n",
      " |          Please avoid the use of argument ``destination`` as it is not\n",
      " |          designed for end-users.\n",
      " |\n",
      " |      Args:\n",
      " |          destination (dict, optional): If provided, the state of module will\n",
      " |              be updated into the dict and the same object is returned.\n",
      " |              Otherwise, an ``OrderedDict`` will be created and returned.\n",
      " |              Default: ``None``.\n",
      " |          prefix (str, optional): a prefix added to parameter and buffer\n",
      " |              names to compose the keys in state_dict. Default: ``''``.\n",
      " |          keep_vars (bool, optional): by default the :class:`~torch.Tensor` s\n",
      " |              returned in the state dict are detached from autograd. If it's\n",
      " |              set to ``True``, detaching will not be performed.\n",
      " |              Default: ``False``.\n",
      " |\n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |\n",
      " |  to(self, *args, **kwargs)\n",
      " |      Move and/or cast the parameters and buffers.\n",
      " |\n",
      " |      This can be called as\n",
      " |\n",
      " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      " |         :noindex:\n",
      " |\n",
      " |      .. function:: to(dtype, non_blocking=False)\n",
      " |         :noindex:\n",
      " |\n",
      " |      .. function:: to(tensor, non_blocking=False)\n",
      " |         :noindex:\n",
      " |\n",
      " |      .. function:: to(memory_format=torch.channels_last)\n",
      " |         :noindex:\n",
      " |\n",
      " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      " |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      " |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      " |      (if given). The integral parameters and buffers will be moved\n",
      " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      " |      pinned memory to CUDA devices.\n",
      " |\n",
      " |      See below for examples.\n",
      " |\n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |\n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): the desired device of the parameters\n",
      " |              and buffers in this module\n",
      " |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      " |              the parameters and buffers in this module\n",
      " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      " |              dtype and device for all parameters and buffers in this module\n",
      " |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      " |              format for 4D parameters and buffers in this module (keyword\n",
      " |              only argument)\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |      Examples::\n",
      " |\n",
      " |          >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      " |          >>> linear = nn.Linear(2, 2)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]])\n",
      " |          >>> linear.to(torch.double)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      " |          >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n",
      " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      " |          >>> cpu = torch.device(\"cpu\")\n",
      " |          >>> linear.to(cpu)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      " |\n",
      " |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      " |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      " |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      " |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      " |\n",
      " |  to_empty(self: ~T, *, device: Union[int, str, torch.device, NoneType], recurse: bool = True) -> ~T\n",
      " |      Move the parameters and buffers to the specified device without copying storage.\n",
      " |\n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): The desired device of the parameters\n",
      " |              and buffers in this module.\n",
      " |          recurse (bool): Whether parameters and buffers of submodules should\n",
      " |              be recursively moved to the specified device.\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      " |\n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |\n",
      " |      Args:\n",
      " |          dst_type (type or string): the desired type\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Move all model parameters and buffers to the XPU.\n",
      " |\n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on XPU while being optimized.\n",
      " |\n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |\n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |  zero_grad(self, set_to_none: bool = True) -> None\n",
      " |      Reset gradients of all model parameters.\n",
      " |\n",
      " |      See similar function under :class:`torch.optim.Optimizer` for more context.\n",
      " |\n",
      " |      Args:\n",
      " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      " |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |\n",
      " |  T_destination = ~T_destination\n",
      " |\n",
      " |  call_super_init = False\n",
      " |\n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "help(YOLO)\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.nn.modules.block import C2f_DWRB, LASPPF, SADown  # Import your custom blocks\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# Register your custom blocks in the global namespace\n",
    "globals()[\"C2f_DWRB\"] = C2f_DWRB\n",
    "globals()[\"LASPPF\"] = LASPPF\n",
    "globals()[\"SADown\"] = SADown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc6df57a-7ccd-4ae4-bfb4-eb6457ded269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C2f_DWRB in globals: True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"C2f_DWRB in globals:\", \"C2f_DWRB\" in globals()) \n",
    "print( \"LASPPF\" in globals()) \n",
    "print(\"SADown\" in globals()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "687d363a-1a8e-4301-ac2d-36b0cd2c04ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saiet\\anaconda3\\Lib\\site-packages\\ultralytics\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import ultralytics\n",
    "print(ultralytics.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66ba9786-15ab-4f13-b74e-db4cb4bfe3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################\n",
      "custom-yolo.yaml\n",
      "WARNING  no model scale passed. Assuming scale='n'.\n",
      "c2f!\n",
      "c2f!\n",
      "c2f!\n",
      "c2f!\n",
      "c2f!\n",
      "c2f!\n",
      "c2f!\n",
      "c2f!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [16, 64, 3, 3], expected input[1, 16, 128, 128] to have 64 channels, but got 16 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom-yolo.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Try loading the YAML config\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ultralytics\\models\\yolo\\model.py:26\u001b[0m, in \u001b[0;36mYOLO.__init__\u001b[1;34m(self, model, task, verbose)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m \u001b[38;5;241m=\u001b[39m new_instance\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Continue with default YOLO initialization\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(model\u001b[38;5;241m=\u001b[39mmodel, task\u001b[38;5;241m=\u001b[39mtask, verbose\u001b[38;5;241m=\u001b[39mverbose)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\model.py:146\u001b[0m, in \u001b[0;36mModel.__init__\u001b[1;34m(self, model, task, verbose)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28m__import__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mos\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUBLAS_WORKSPACE_CONFIG\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:4096:8\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# to avoid deterministic warnings\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Path(model)\u001b[38;5;241m.\u001b[39msuffix \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.yml\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[1;32m--> 146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(model, task\u001b[38;5;241m=\u001b[39mtask, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load(model, task\u001b[38;5;241m=\u001b[39mtask)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\model.py:257\u001b[0m, in \u001b[0;36mModel._new\u001b[1;34m(self, cfg, task, model, verbose)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg \u001b[38;5;241m=\u001b[39m cfg\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m=\u001b[39m task \u001b[38;5;129;01mor\u001b[39;00m guess_model_task(cfg_dict)\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m (model \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_smart_load(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m))(cfg_dict, verbose\u001b[38;5;241m=\u001b[39mverbose \u001b[38;5;129;01mand\u001b[39;00m RANK \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# build model\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverrides[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverrides[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:338\u001b[0m, in \u001b[0;36mDetectionModel.__init__\u001b[1;34m(self, cfg, ch, nc, verbose)\u001b[0m\n\u001b[0;32m    335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(x)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mone2many\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(x)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m, (Segment, Pose, OBB)) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(x)\n\u001b[1;32m--> 338\u001b[0m m\u001b[38;5;241m.\u001b[39mstride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([s \u001b[38;5;241m/\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m _forward(torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, ch, s, s))])  \u001b[38;5;66;03m# forward\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mstride\n\u001b[0;32m    340\u001b[0m m\u001b[38;5;241m.\u001b[39mbias_init()  \u001b[38;5;66;03m# only run once\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:336\u001b[0m, in \u001b[0;36mDetectionModel.__init__.<locals>._forward\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend2end:\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(x)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mone2many\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 336\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(x)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m, (Segment, Pose, OBB)) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(x)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:117\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:135\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[1;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[1;32m--> 135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_once(x, profile, visualize, embed)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:156\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[1;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m--> 156\u001b[0m x \u001b[38;5;241m=\u001b[39m m(x)  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m    157\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ultralytics\\nn\\modules\\block.py:1432\u001b[0m, in \u001b[0;36mSADown.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1430\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m   1431\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through SADown.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1432\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ultralytics\\nn\\modules\\conv.py:51\u001b[0m, in \u001b[0;36mConv.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     50\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply convolution, batch normalization and activation to input tensor.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x)))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[0;32m    551\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [16, 64, 3, 3], expected input[1, 16, 128, 128] to have 64 channels, but got 16 channels instead"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"custom-yolo.yaml\")  # Try loading the YAML config\n",
    "print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a35b96b-ba53-4923-a443-3b91e1cbbe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.nn.modules.block import C2f_DWRB as C2f\n",
    "from ultralytics.nn.modules.block import C2f_DWRB\n",
    "print(C2f_DWRB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52299a7d-a1f0-4f6c-8252-7947a329d1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.nn.modules.block import SADown as Downsample\n",
    "from ultralytics.nn.modules.block import SADown\n",
    "print(SADown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fcdf60-7fc2-4071-af54-3e4413901057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.nn.modules.block import LASPPF as SPPF\n",
    "from ultralytics.nn.modules.block import LASPPF\n",
    "print(LASPPF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7097ca79-6275-4221-8007-9ff7e19539fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load default YOLO model\n",
    "model = YOLO(\"yolov8n.yaml\")  # or yolov8s.yaml, yolov8m.yaml, etc.\n",
    "\n",
    "# Print YAML file path\n",
    "print(model.yaml)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f6cabb-070d-4ba5-83b2-586e88b4966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from ultralytics.nn.modules.block import C2f, SPPF  # Default YOLOv8 blocks\n",
    "from ultralytics.nn.modules.block import C2f_DWRB, LASPPF, SADown  # Import custom blocks\n",
    "\n",
    "# Register custom modules so YOLO can recognize them\n",
    "import torch.nn as nn\n",
    "\n",
    "globals()[\"C2f_DWRB\"] = C2f_DWRB  # Register C2f_DWRB globally\n",
    "globals()[\"LASPPF\"] = LASPPF  # Register LASPPF globally\n",
    "globals()[\"SADown\"] = SADown  # Register SADown globally\n",
    "\n",
    "# Load modified YOLO model with custom YAML\n",
    "#model = YOLO(\"yolov8.yaml\")\n",
    "\n",
    "# Print YAML path to verify successful loading\n",
    "#print(\"Model YAML Path:\", model.yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4de764-ec8d-49cb-8edf-8fb02e34123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.models.yolo import YOLO\n",
    "\n",
    "# Load YOLO with your modified config\n",
    "model = YOLO(\"custom-yolo.yaml\")  # Replace with your model config if needed\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151646d3-eb40-4f9e-bd1e-5cb981070ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.models.yolo import YOLO\n",
    "from ultralytics.nn.modules.block import C2f, SPPF  # Import original YOLO blocks\n",
    "\n",
    "# Import your custom blocks\n",
    "from ultralytics.nn.modules import C2f_DWRB, SADown, LASPPF\n",
    "model = YOLO(\"yolov8n.yaml\")\n",
    "import torch.nn as nn\n",
    "\n",
    "# Loop through YOLO model layers and replace specific blocks\n",
    "for name, module in model.model.named_children():\n",
    "    if isinstance(module, C2f):\n",
    "        print(\"###########################################################\")\n",
    "        setattr(model.model, name, C2f_DWRB(module.cv1.conv.in_channels, module.cv2.conv.out_channels))\n",
    "    elif isinstance(module, SPPF):\n",
    "        setattr(model.model, name, LASPPF(module.cv1.conv.in_channels, module.cv2.conv.out_channels))\n",
    "    elif isinstance(module, nn.Conv2d) and module.kernel_size == (3, 3) and module.stride == (2, 2):\n",
    "        setattr(model.model, name, SADown(module.in_channels, module.out_channels))\n",
    "\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a56c96c-29bf-46af-b943-a1aaa16649e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20fe4b5-3d76-47ec-bb68-fbc9375e6cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eeb07f-4679-44a2-b650-82ada1a1040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = YOLO(\"custom-yolo.yaml\")\n",
    "print(any(isinstance(m, C2f) for m in model.model.modules()))  # Should print True\n",
    "print(any(isinstance(m, LASPPF) for m in model.model.modules()))  # Should print True\n",
    "print(any(isinstance(m, SADown) for m in model.model.modules()))  # Should print True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9922cc88-3585-46ce-b039-6080772a3ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
